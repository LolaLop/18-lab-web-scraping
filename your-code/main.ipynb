{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:44:39.207014Z",
     "start_time": "2020-08-13T06:44:36.920271Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "# import random\n",
    "# import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:44:40.385778Z",
     "start_time": "2020-08-13T06:44:40.380930Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:44:44.207494Z",
     "start_time": "2020-08-13T06:44:41.477588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trending  developers on GitHub today · GitHub']\n"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "print(soup.title.contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:44:45.922392Z",
     "start_time": "2020-08-13T06:44:45.860392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Francois Zaninotto', 'Graham Campbell', 'Chocobozzz', 'Seth Vargo', 'Kazuho Oku', 'Michael Lynch', 'Gleb Bahmutov', 'Barry vd. Heuvel', 'Mark Story', 'Adam Wathan', 'Sindre Sorhus', 'Jason Quense', 'Stephen Celis', 'Wojciech Maj', 'Sam Sam', 'Filip W', 'Gerasimos (Makis) Maropoulos', 'Markus Unterwaditzer', 'KUOKA Yusuke', 'Travis Tidwell', 'Raúl Gómez Acuña', 'Michael Grosser', 'Felix Yan', 'Felipe Fialho', 'Evan Wallace']\n"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "\n",
    "# Find all the developers\n",
    "results = soup.find_all('h1', attrs={'class':'h3 lh-condensed'})\n",
    "\n",
    "# Check each result to obtain only the name and add it to the names' list\n",
    "names = [ result.find('a').text.replace('\\n', '').replace('  ','') for result in results]\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:44:48.453460Z",
     "start_time": "2020-08-13T06:44:48.447273Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:44:51.257918Z",
     "start_time": "2020-08-13T06:44:49.790222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prefect', 'AnimeGANv2', 'IntelOwl', 'sherlock', 'mmdetection3d', 'pandas', 'White-box-Cartoonization', 'eat_pytorch_in_20_days', 'Super-mario-bros-PPO-pytorch', 'django', 'Summer2021-Internships', 'azure-sdk-for-python', 'scrapy', 'bert', 'inltk', 'AnimeGAN', 'CS-Notes', 'wagtail', 'localstack', 'magic-python', 'dagster', 'qiling', 'InvoiceNet', 'pyinstaller', 'fawkes']\n"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "\n",
    "# I have understood that the repository names is the last part as the first part is the user name\n",
    "\n",
    "# Parse the URL\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "# Find all the repositories\n",
    "results = soup.find_all('h1', attrs={'class':'h3 lh-condensed'})\n",
    "\n",
    "# Check each result to obtain only the name and add it to the names' list\n",
    "repos_names = [result.find('a').contents[-1].replace('\\n', '').replace(' ','') for result in results]\n",
    "\n",
    "print(repos_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:44:52.643794Z",
     "start_time": "2020-08-13T06:44:52.636403Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:44:54.362394Z",
     "start_time": "2020-08-13T06:44:53.527065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG', '//upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png', '//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg', '//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG', '//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG', '//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/4/44/The_Walt_Disney_Company_Logo.svg/120px-The_Walt_Disney_Company_Logo.svg.png', '//upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/30px-Animation_disc.svg.png', '//upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/29px-P_vip.svg.png', '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg', '//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/30px-Video-x-generic.svg.png', '//upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png', '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/30px-Blank_television_set.svg.png', '//upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/30px-Flag_of_the_United_States.svg.png']\n"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "\n",
    "# Parse the URL\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "# Find all the images\n",
    "results = soup.find_all('a', attrs={'class':'image'})\n",
    "\n",
    "images_links = [result.find('img')['src'] for result in results]\n",
    "\n",
    "print(images_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T07:05:42.774714Z",
     "start_time": "2020-08-13T07:05:42.770237Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T07:05:52.835218Z",
     "start_time": "2020-08-13T07:05:52.627992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#Snakes', '#Ancient_Greece', '#Media_and_entertainment', '#Computing', '#Engineering', '#Roller_coasters', '#Vehicles', '#Weaponry', '#People', '#Other_uses', '#See_also', '/wiki/Pythonidae', '/wiki/Python_(genus)', '/wiki/Python_(mythology)', '/wiki/Python_of_Aenus', '/wiki/Python_(painter)', '/wiki/Python_of_Byzantium', '/wiki/Python_of_Catana', '/wiki/Python_(film)', '/wiki/Pythons_2', '/wiki/Monty_Python', '/wiki/Python_(Monty)_Pictures', '/wiki/Python_(programming_language)', '/wiki/CPython', '/wiki/CMU_Common_Lisp', '/wiki/PERQ#PERQ_3', '/wiki/Python_(Busch_Gardens_Tampa_Bay)', '/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)', '/wiki/Python_(Efteling)', '/wiki/Python_(automobile_maker)', '/wiki/Python_(Ford_prototype)', '/wiki/Colt_Python', '/wiki/Python_(missile)', '/wiki/Python_(nuclear_primary)', '/wiki/Python_Anghelo', '/wiki/PYTHON', '/wiki/Cython', '/wiki/Pyton', '/wiki/Category:Disambiguation_pages', '/wiki/Category:Disambiguation_pages_with_short_descriptions', '/wiki/Category:Short_description_is_different_from_Wikidata', '/wiki/Category:All_article_disambiguation_pages', '/wiki/Category:All_disambiguation_pages', '/wiki/Category:Animal_common_name_disambiguation_pages', '/wiki/Special:MyTalk', '/wiki/Special:MyContributions', '/w/index.php?title=Special:CreateAccount&returnto=Python', '/w/index.php?title=Special:UserLogin&returnto=Python', '/wiki/Python', '/wiki/Talk:Python', '/wiki/Python', '/w/index.php?title=Python&action=edit', '/w/index.php?title=Python&action=history', '/wiki/Main_Page', '/wiki/Wikipedia:Contents', '/wiki/Portal:Current_events', '/wiki/Special:Random', '/wiki/Wikipedia:About', '//en.wikipedia.org/wiki/Wikipedia:Contact_us', 'https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en', '//shop.wikimedia.org', '/wiki/Help:Contents', '/wiki/Wikipedia:Community_portal', '/wiki/Special:RecentChanges', '/wiki/Wikipedia:File_Upload_Wizard', '/wiki/Special:WhatLinksHere/Python', '/wiki/Special:RecentChangesLinked/Python', '/wiki/Wikipedia:File_Upload_Wizard', '/wiki/Special:SpecialPages', '/w/index.php?title=Python&oldid=963092579', '/w/index.php?title=Python&action=info', '/w/index.php?title=Special:CiteThisPage&page=Python&id=963092579&wpFormIdentifier=titleform', 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452', '/w/index.php?title=Special:ElectronPdf&page=Python&action=show-download-screen', '/w/index.php?title=Python&printable=yes', 'https://commons.wikimedia.org/wiki/Category:Python', 'https://af.wikipedia.org/wiki/Python', 'https://als.wikipedia.org/wiki/Python', 'https://ar.wikipedia.org/wiki/%D8%A8%D8%A7%D9%8A%D8%AB%D9%88%D9%86', 'https://az.wikipedia.org/wiki/Python', 'https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)', 'https://be.wikipedia.org/wiki/Python', 'https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)', 'https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)', 'https://da.wikipedia.org/wiki/Python', 'https://de.wikipedia.org/wiki/Python', 'https://eo.wikipedia.org/wiki/Pitono_(apartigilo)', 'https://eu.wikipedia.org/wiki/Python_(argipena)', 'https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86', 'https://fr.wikipedia.org/wiki/Python', 'https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0', 'https://hr.wikipedia.org/wiki/Python_(razdvojba)', 'https://io.wikipedia.org/wiki/Pitono', 'https://id.wikipedia.org/wiki/Python', 'https://ia.wikipedia.org/wiki/Python_(disambiguation)', 'https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)', 'https://it.wikipedia.org/wiki/Python_(disambigua)', 'https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F', 'https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)', 'https://kg.wikipedia.org/wiki/Mboma_(nyoka)', 'https://la.wikipedia.org/wiki/Python_(discretiva)', 'https://lb.wikipedia.org/wiki/Python', 'https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)', 'https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)', 'https://nl.wikipedia.org/wiki/Python', 'https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3', 'https://no.wikipedia.org/wiki/Pyton', 'https://pl.wikipedia.org/wiki/Pyton', 'https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)', 'https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)', 'https://sk.wikipedia.org/wiki/Python', 'https://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%B2%D0%B8%D1%88%D0%B5%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%B0_%D0%BE%D0%B4%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%86%D0%B0)', 'https://sh.wikipedia.org/wiki/Python', 'https://fi.wikipedia.org/wiki/Python', 'https://sv.wikipedia.org/wiki/Pyton', 'https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99', 'https://tr.wikipedia.org/wiki/Python', 'https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD', 'https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86', 'https://vi.wikipedia.org/wiki/Python', 'https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)', '//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License', 'https://foundation.wikimedia.org/wiki/Privacy_policy', '/wiki/Wikipedia:About', '/wiki/Wikipedia:General_disclaimer', '//en.wikipedia.org/wiki/Wikipedia:Contact_us', 'https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute', 'https://stats.wikimedia.org/#/en.wikipedia.org', 'https://foundation.wikimedia.org/wiki/Cookie_statement', '//en.m.wikipedia.org/w/index.php?title=Python&mobileaction=toggle_view_mobile', 'https://wikimediafoundation.org/', 'https://www.mediawiki.org/']\n"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "\n",
    "# Parse the URL\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "# Find all the links\n",
    "results = soup.find_all('li')\n",
    "\n",
    "links = []\n",
    "\n",
    "for result in results:\n",
    "    \n",
    "    # Some of the results do not have a child a with attribute href. In this case, skip it and continue\n",
    "    try:\n",
    "        links.append(result.find('a')['href'])\n",
    "    except:\n",
    "        continue\n",
    "     \n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:44:59.557948Z",
     "start_time": "2020-08-13T06:44:59.552017Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:45:03.588382Z",
     "start_time": "2020-08-13T06:45:00.646506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title 54 - National Park Service and Related Programs']\n"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "\n",
    "# Parse the URL\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "# Find all the links\n",
    "results = soup.find_all('div', attrs={'class':'usctitlechanged'})\n",
    "\n",
    "changed_titles = [ result.text.replace('\\n','').replace('  ','').replace(' ٭','') for result in results]\n",
    "\n",
    "print(changed_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:45:03.724528Z",
     "start_time": "2020-08-13T06:45:03.717521Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:45:04.186458Z",
     "start_time": "2020-08-13T06:45:03.930726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EUGENE PALMER', 'RAFAEL CARO-QUINTERO', 'ROBERT WILLIAM FISHER', 'BHADRESHKUMAR CHETANBHAI PATEL', 'ALEJANDRO ROSALES CASTILLO', 'ARNOLDO JIMENEZ', 'JASON DEREK BROWN', 'YASER ABDEL SAID', 'ALEXIS FLORES', 'SANTIAGO VILLALBA MEDEROS']\n"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "\n",
    "# Parse the URL\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "# Find all the repositories\n",
    "results = soup.find_all('h3', attrs={'class':'title'})\n",
    "\n",
    "topten = [ result.find('a').text for result in results ]\n",
    "\n",
    "print(topten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:45:05.968781Z",
     "start_time": "2020-08-13T06:45:05.960401Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:45:07.520498Z",
     "start_time": "2020-08-13T06:45:06.807167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>06:03:50</td>\n",
       "      <td>19.42N</td>\n",
       "      <td>155.63W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>05:45:18</td>\n",
       "      <td>60.30N</td>\n",
       "      <td>153.48W</td>\n",
       "      <td>SOUTHERN ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>05:23:03</td>\n",
       "      <td>30.41N</td>\n",
       "      <td>94.54E</td>\n",
       "      <td>EASTERN XIZANG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>05:21:28</td>\n",
       "      <td>40.71N</td>\n",
       "      <td>48.44E</td>\n",
       "      <td>AZERBAIJAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>04:42:48</td>\n",
       "      <td>41.96N</td>\n",
       "      <td>75.28E</td>\n",
       "      <td>KYRGYZSTAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>04:27:58</td>\n",
       "      <td>30.44N</td>\n",
       "      <td>95.01E</td>\n",
       "      <td>EASTERN XIZANG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>04:18:26</td>\n",
       "      <td>36.73N</td>\n",
       "      <td>120.83W</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>03:56:26</td>\n",
       "      <td>28.29S</td>\n",
       "      <td>69.67W</td>\n",
       "      <td>ATACAMA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>03:52:30</td>\n",
       "      <td>35.40N</td>\n",
       "      <td>140.50E</td>\n",
       "      <td>NEAR EAST COAST OF HONSHU, JAPAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>03:43:39</td>\n",
       "      <td>38.76N</td>\n",
       "      <td>40.22E</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>03:33:01</td>\n",
       "      <td>34.80S</td>\n",
       "      <td>71.69W</td>\n",
       "      <td>LIBERTADOR O'HIGGINS, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>03:28:32</td>\n",
       "      <td>33.24N</td>\n",
       "      <td>115.68W</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>03:09:30</td>\n",
       "      <td>5.96S</td>\n",
       "      <td>147.87E</td>\n",
       "      <td>EASTERN NEW GUINEA REG., P.N.G.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>03:05:58</td>\n",
       "      <td>19.22N</td>\n",
       "      <td>155.39W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>03:03:09</td>\n",
       "      <td>54.81N</td>\n",
       "      <td>160.31W</td>\n",
       "      <td>ALASKA PENINSULA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>02:54:02</td>\n",
       "      <td>19.24N</td>\n",
       "      <td>155.37W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>02:49:41</td>\n",
       "      <td>58.67N</td>\n",
       "      <td>141.61W</td>\n",
       "      <td>OFF COAST OF SOUTHEASTERN ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>02:35:34</td>\n",
       "      <td>35.30N</td>\n",
       "      <td>140.27E</td>\n",
       "      <td>NEAR EAST COAST OF HONSHU, JAPAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>02:34:17</td>\n",
       "      <td>62.24N</td>\n",
       "      <td>124.43W</td>\n",
       "      <td>NORTHWEST TERRITORIES, CANADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>02:29:54</td>\n",
       "      <td>38.12N</td>\n",
       "      <td>118.15W</td>\n",
       "      <td>NEVADA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date       time latitude longitude                            region\n",
       "0   2020-08-13   06:03:50   19.42N   155.63W          ISLAND OF HAWAII, HAWAII\n",
       "1   2020-08-13   05:45:18   60.30N   153.48W                   SOUTHERN ALASKA\n",
       "2   2020-08-13   05:23:03   30.41N    94.54E                    EASTERN XIZANG\n",
       "3   2020-08-13   05:21:28   40.71N    48.44E                        AZERBAIJAN\n",
       "4   2020-08-13   04:42:48   41.96N    75.28E                        KYRGYZSTAN\n",
       "5   2020-08-13   04:27:58   30.44N    95.01E                    EASTERN XIZANG\n",
       "6   2020-08-13   04:18:26   36.73N   120.83W                CENTRAL CALIFORNIA\n",
       "7   2020-08-13   03:56:26   28.29S    69.67W                    ATACAMA, CHILE\n",
       "8   2020-08-13   03:52:30   35.40N   140.50E  NEAR EAST COAST OF HONSHU, JAPAN\n",
       "9   2020-08-13   03:43:39   38.76N    40.22E                    EASTERN TURKEY\n",
       "10  2020-08-13   03:33:01   34.80S    71.69W       LIBERTADOR O'HIGGINS, CHILE\n",
       "11  2020-08-13   03:28:32   33.24N   115.68W               SOUTHERN CALIFORNIA\n",
       "12  2020-08-13   03:09:30    5.96S   147.87E   EASTERN NEW GUINEA REG., P.N.G.\n",
       "13  2020-08-13   03:05:58   19.22N   155.39W          ISLAND OF HAWAII, HAWAII\n",
       "14  2020-08-13   03:03:09   54.81N   160.31W                  ALASKA PENINSULA\n",
       "15  2020-08-13   02:54:02   19.24N   155.37W          ISLAND OF HAWAII, HAWAII\n",
       "16  2020-08-13   02:49:41   58.67N   141.61W  OFF COAST OF SOUTHEASTERN ALASKA\n",
       "17  2020-08-13   02:35:34   35.30N   140.27E  NEAR EAST COAST OF HONSHU, JAPAN\n",
       "18  2020-08-13   02:34:17   62.24N   124.43W     NORTHWEST TERRITORIES, CANADA\n",
       "19  2020-08-13   02:29:54   38.12N   118.15W                            NEVADA"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#your code\n",
    "\n",
    "# Parse the URL\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "# Find all the repositories\n",
    "results = soup.find('tbody').find_all('tr')\n",
    "\n",
    "# List containing all the information\n",
    "earthquakes = []\n",
    "\n",
    "# Auxiliar variable\n",
    "count = 1\n",
    "\n",
    "for result in results:\n",
    "\n",
    "    coordinates_num = result.find_all('td', attrs={'class':'tabev1'})\n",
    "    coordinates_NS = result.find_all('td', attrs={'class':'tabev2'})\n",
    "    latitude = coordinates_num[0].text.replace('\\xa0','') + coordinates_NS[0].text.replace('\\xa0','')\n",
    "    longitude = coordinates_num[1].text.replace('\\xa0','') + coordinates_NS[1].text.replace('\\xa0','')\n",
    "    region = result.find('td', attrs={'class':'tb_region'}).text.replace('\\xa0','')\n",
    "    date = result.find('td', attrs={'class':'tabev6'}).find('b').find('a').text[:10]\n",
    "    time = result.find('td', attrs={'class':'tabev6'}).find('b').find('a').text[12:-2]\n",
    "    \n",
    "    # Add all the elements to the list\n",
    "    earthquakes.append((date, time, latitude, longitude, region))\n",
    "    \n",
    "    if count == 20:\n",
    "        break\n",
    "    \n",
    "    count += 1\n",
    "\n",
    "# Import the output list to a dataframe structure\n",
    "df = pd.DataFrame(earthquakes, columns=['date', 'time', 'latitude', 'longitude', 'region'])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:45:10.370936Z",
     "start_time": "2020-08-13T06:45:10.365898Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:45:11.356968Z",
     "start_time": "2020-08-13T06:45:11.347190Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "\n",
    "# I don't have a twitter account neither the credentials to access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:45:13.205272Z",
     "start_time": "2020-08-13T06:45:13.199766Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T06:45:13.710761Z",
     "start_time": "2020-08-13T06:45:13.706288Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "\n",
    "# I don't have a twitter account neither the credentials to access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T07:10:35.846382Z",
     "start_time": "2020-08-13T07:10:35.839893Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T07:18:09.463784Z",
     "start_time": "2020-08-13T07:18:09.182895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('English', '6137000+'), ('æ\\x97¥æ\\x9c¬èª\\x9e', '1222000+'), ('EspaÃ±ol', '1617000+'), ('Deutsch', '2467000+'), ('ÐÑ\\x83Ñ\\x81Ñ\\x81ÐºÐ¸Ð¹', '1651000+'), ('FranÃ§ais', '2241000+'), ('Italiano', '1627000+'), ('ä¸\\xadæ\\x96\\x87', '1136000+'), ('PortuguÃªs', '1041000+'), ('Polski', '1423000+')]\n"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "\n",
    "# Parse the URL\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "# Find all the languages with the number of articles that appear in the main page\n",
    "results = soup.find_all('a', attrs={'class':'link-box'})\n",
    "\n",
    "# List containing all the information\n",
    "wiki = []\n",
    "\n",
    "for result in results:\n",
    "    lang = result.find('strong').text.replace('\\xa0','')\n",
    "    num_articles = result.find('small').find('bdi').text.replace('\\xa0','')\n",
    "    \n",
    "    wiki.append((lang, num_articles))\n",
    "\n",
    "print( wiki )\n",
    "\n",
    "# To do: check for special characters in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T07:22:53.617872Z",
     "start_time": "2020-08-13T07:22:53.613691Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T07:25:05.252709Z",
     "start_time": "2020-08-13T07:25:05.049009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Business and economy', 'Crime and justice', 'Defence', 'Education', 'Environment', 'Government', 'Government spending', 'Health', 'Mapping', 'Society', 'Towns and cities', 'Transport']\n"
     ]
    }
   ],
   "source": [
    "#your code \n",
    "\n",
    "# Parse the URL\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "# List of the different kind of datasets available\n",
    "results = soup.find_all('h3', attrs={'class':'govuk-heading-s dgu-topics__heading'})\n",
    "\n",
    "datasets_types = [ result.text for result in results ]\n",
    "\n",
    "print(datasets_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T07:30:15.031966Z",
     "start_time": "2020-08-13T07:30:15.027337Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T07:42:49.680955Z",
     "start_time": "2020-08-13T07:42:49.183521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>native speakers in millions 2007 (2010)</th>\n",
       "      <th>percentage of the world population</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mandarin (entire branch)</td>\n",
       "      <td>935 (955)</td>\n",
       "      <td>14.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>390 (405)</td>\n",
       "      <td>5.85%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "      <td>365 (360)</td>\n",
       "      <td>5.52%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi[a]</td>\n",
       "      <td>295 (310)</td>\n",
       "      <td>4.46%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>280 (295)</td>\n",
       "      <td>4.23%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>205 (215)</td>\n",
       "      <td>3.08%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>200 (205)</td>\n",
       "      <td>3.05%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Russian</td>\n",
       "      <td>160 (155)</td>\n",
       "      <td>2.42%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>125 (125)</td>\n",
       "      <td>1.92%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Punjabi</td>\n",
       "      <td>95 (100)</td>\n",
       "      <td>1.44%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      language native speakers in millions 2007 (2010)  \\\n",
       "rank                                                                     \n",
       "1     Mandarin (entire branch)                               935 (955)   \n",
       "2                      Spanish                               390 (405)   \n",
       "3                      English                               365 (360)   \n",
       "4                     Hindi[a]                               295 (310)   \n",
       "5                       Arabic                               280 (295)   \n",
       "6                   Portuguese                               205 (215)   \n",
       "7                      Bengali                               200 (205)   \n",
       "8                      Russian                               160 (155)   \n",
       "9                     Japanese                               125 (125)   \n",
       "10                     Punjabi                                95 (100)   \n",
       "\n",
       "     percentage of the world population  \n",
       "rank                                     \n",
       "1                                 14.1%  \n",
       "2                                 5.85%  \n",
       "3                                 5.52%  \n",
       "4                                 4.46%  \n",
       "5                                 4.23%  \n",
       "6                                 3.08%  \n",
       "7                                 3.05%  \n",
       "8                                 2.42%  \n",
       "9                                 1.92%  \n",
       "10                                1.44%  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "\n",
    "# Parse the URL\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "# Top 10 languages\n",
    "tables = soup.find_all('table', attrs={'class':'wikitable sortable'})\n",
    "\n",
    "# There are two tables, the one we are looking for is the second one\n",
    "table = tables[1].find('tbody').find_all('tr')\n",
    "\n",
    "output = []\n",
    "# We are only interested in the top 10 languages\n",
    "for i in range(1,11):\n",
    "    info = table[i].find_all('td')\n",
    "    rank = info[0].text\n",
    "    lang = info[1].text\n",
    "    num_speakers = info[2].text\n",
    "    perc_speakers = info[3].text.replace('\\n','')\n",
    "    \n",
    "    # Append all the values to a list\n",
    "    output.append((rank, lang, num_speakers, perc_speakers))\n",
    "\n",
    "# Convert the output into a dataframe\n",
    "df = pd.DataFrame(output, columns=['rank', 'language', 'native speakers in millions 2007 (2010)', 'percentage of the world population'])\n",
    "\n",
    "df.set_index('rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T08:50:56.309642Z",
     "start_time": "2020-08-13T08:50:56.304284Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T09:09:40.912974Z",
     "start_time": "2020-08-13T09:09:39.071035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>release</th>\n",
       "      <th>director</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>1994</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1972</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Godfather: Part II</td>\n",
       "      <td>1974</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>2008</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>1957</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      movie release              director  stars\n",
       "0  The Shawshank Redemption    1994        Frank Darabont    9.2\n",
       "1             The Godfather    1972  Francis Ford Coppola    9.1\n",
       "2    The Godfather: Part II    1974  Francis Ford Coppola    9.0\n",
       "3           The Dark Knight    2008     Christopher Nolan    9.0\n",
       "4              12 Angry Men    1957          Sidney Lumet    8.9"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "\n",
    "# Parse the URL\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "# Top 250 movies\n",
    "table = soup.find('table', attrs={'class':'chart full-width'}).find_all('tr')\n",
    "\n",
    "movies = []\n",
    "\n",
    "# Loop over the table to get the information requested\n",
    "for i in range(1, 251):\n",
    "    director = table[i].find_all('a')[1]['title'].split(',')[0].replace(' (dir.)','')\n",
    "    # actors = table[i].find_all('a')[1]['title'].split(',')[1:] - not requested\n",
    "    movie = table[i].find_all('a')[1].text\n",
    "    release = table[i].find('span', attrs={'class':'secondaryInfo'}).text.replace('(','').replace(')','')\n",
    "    stars = eval(table[i].find('td', attrs={'class':'ratingColumn'}).text.replace('\\n',''))\n",
    "    \n",
    "    # Append all the information to a list\n",
    "    movies.append((movie, release, director, stars))\n",
    "\n",
    "# Transform to a dataframe\n",
    "df = pd.DataFrame(movies, columns=['movie', 'release', 'director', 'stars'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T09:09:53.711748Z",
     "start_time": "2020-08-13T09:09:53.706382Z"
    }
   },
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T09:12:34.457502Z",
     "start_time": "2020-08-13T09:12:32.752957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>The Big Lebowski</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Toy Story</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Logan</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Det sjunde inseglet</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Joker</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Singin' in the Rain</td>\n",
       "      <td>1952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Terminator 2: Judgment Day</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Le salaire de la peur</td>\n",
       "      <td>1953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Tonari no Totoro</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>The Lives of Others</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          movie release\n",
       "188            The Big Lebowski    1998\n",
       "80                    Toy Story    1995\n",
       "217                       Logan    2017\n",
       "158         Det sjunde inseglet    1957\n",
       "57                        Joker    2019\n",
       "104         Singin' in the Rain    1952\n",
       "37   Terminator 2: Judgment Day    1991\n",
       "212       Le salaire de la peur    1953\n",
       "145            Tonari no Totoro    1988\n",
       "58          The Lives of Others    2006"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "\n",
    "# Parse the URL\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "# Top 250 movies\n",
    "table = soup.find('table', attrs={'class':'chart full-width'}).find_all('tr')\n",
    "\n",
    "movies = []\n",
    "\n",
    "# Loop over the table to get the information requested\n",
    "for i in range(1, 251):\n",
    "    movie = table[i].find_all('a')[1].text\n",
    "    release = table[i].find('span', attrs={'class':'secondaryInfo'}).text.replace('(','').replace(')','')\n",
    "    \n",
    "    # Append all the information to a list\n",
    "    movies.append((movie, release))\n",
    "\n",
    "# Transform to a dataframe\n",
    "df = pd.DataFrame(movies, columns=['movie', 'release'])\n",
    "\n",
    "output = df.sample(10)\n",
    "\n",
    "output\n",
    "\n",
    "# I don't know how to find the brief summary unless I go inside each movie. \n",
    "# Not sure if this is what was requested\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T09:13:54.810120Z",
     "start_time": "2020-08-13T09:13:31.578646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the city:Barcelona\n"
     ]
    }
   ],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T09:22:47.533706Z",
     "start_time": "2020-08-13T09:22:47.364305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather report in Barcelona:\n",
      " - temperature: 28.28\n",
      " - wind speed: 1.5\n",
      " - weather: Clouds\n",
      " - description: few clouds\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "\n",
    "# Parse the URL\n",
    "r = requests.get(url)\n",
    "\n",
    "temp = r.json()['main']['temp']\n",
    "wind = r.json()['wind']['speed']\n",
    "description = r.json()['weather'][0]['description']\n",
    "weather = r.json()['weather'][0]['main']\n",
    "\n",
    "print(f'Weather report in {city}:\\n - temperature: {temp}\\n - wind speed: {wind}\\n - weather: {weather}\\n - description: {description}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T09:23:19.587366Z",
     "start_time": "2020-08-13T09:23:19.582289Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T09:34:14.638210Z",
     "start_time": "2020-08-13T09:34:14.469496Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>price (£)</th>\n",
       "      <th>availability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>51.77</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>53.74</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>50.10</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>47.82</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>54.23</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    name price (£) availability\n",
       "0                   A Light in the Attic     51.77     In stock\n",
       "1                     Tipping the Velvet     53.74     In stock\n",
       "2                             Soumission     50.10     In stock\n",
       "3                          Sharp Objects     47.82     In stock\n",
       "4  Sapiens: A Brief History of Humankind     54.23     In stock"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "\n",
    "#your code\n",
    "\n",
    "# Parse the URL\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "#Find the articles\n",
    "articles = soup.find_all('article', attrs={'class':'product_pod'})\n",
    "\n",
    "output = []\n",
    "\n",
    "# Loop over all the products to obtain the information requested\n",
    "for article in articles:\n",
    "    name = article.find('h3').find('a')['title']\n",
    "    price = article.find('div', attrs={'class':'product_price'}).find('p', attrs={'class':'price_color'}).text.replace('Â£','')\n",
    "    availability = article.find('div', attrs={'class':'product_price'}).find('p', attrs={'class':'instock availability'}).text.replace('\\n','').replace('  ','')\n",
    "    \n",
    "    # Append the information to the output list\n",
    "    output.append((name, price, availability))\n",
    "    \n",
    "# Convert into a DataFrame\n",
    "df = pd.DataFrame(output, columns=['name', 'price (£)', 'availability'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
